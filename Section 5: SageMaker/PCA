# ðŸ“Š PCA (Principal Component Analysis) â€” Exam Table

| **Aspect**              | **What to Know**                                                            |
| ----------------------- | --------------------------------------------------------------------------- |
| **Type**                | Unsupervised â€” Dimensionality Reduction                                     |
| **Goal**                | Reduce high-dimensional data into fewer components while retaining variance |
| **Input Format**        | `CSV` or `RecordIO-Protobuf`                                                |
| **Input Mode**          | `File` or `Pipe`                                                            |
| **Core Algorithm**      | Uses **Singular Value Decomposition (SVD)** on the **covariance matrix**    |
| **Main Use Case**       | Reduce features before training to avoid overfitting / improve performance  |
| **Key Hyperparameters** | `algorithm_mode` (`regular` or `randomized`), `subtract_mean`               |
| **Modes**               | `regular` â†’ sparse/moderate data, `randomized` â†’ large data                 |
| **Output**              | Top-N **principal components** (no labels, just dimensions)                 |
| **Instance Types**      | âœ… CPU or GPU supported (no fixed recommendation â€” try both)                |


# ðŸ”¥ What to Memorise for the Exam
PCA is used for dimensionality reduction, not prediction

Itâ€™s unsupervised, doesnâ€™t require labels

Returns components, not feature names

SageMaker supports both regular and randomized modes

Hyperparams to know: algorithm_mode, subtract_mean

Can use GPU or CPU, depending on dataset size and sparsity

# âœ… Exam Clue:

If a question asks about reducing features or removing irrelevant dimensions, the answer is probably PCA.
