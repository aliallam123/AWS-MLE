# SageMaker Automatic Model Tuning (AMT) â€“ AWS MLE-A Exam

## ğŸ§  What It Is

- SageMaker AMT automates **hyperparameter tuning** using parallel training jobs
- Learns over time â†’ uses **Bayesian optimisation**, not brute-force grid search
- Trains multiple models with different hyperparameter combos to **optimise a metric**

## ğŸ¯ Key Best Practices

| Practice                        | Why it matters                                      |
|---------------------------------|-----------------------------------------------------|
| âœ… Limit hyperparameters         | Too many â†’ exponential cost & time                  |
| âœ… Use small search ranges       | Large ranges â†’ wasted resources                     |
| âœ… Use **log scale** when needed | For values like learning rate (e.g. 0.001â€“0.1)      |
| âœ… Keep concurrent jobs low      | Learning works better **sequentially**              |
| âœ… Report correct metric         | Needed if using custom training scripts             |

## ğŸš€ Answer Clues for the Exam

- If question says **â€œefficient tuningâ€** â†’ choose **Bayesian** or **SageMaker AMT**
- If question asks how to **reduce cost/time** â†’ tune fewer hyperparams, use small ranges
- If question says **parallel tuning isn't working well** â†’ reduce concurrency
- Log scale = good for **learning rate**, **regularisation**, etc.
- Grid/random search = **less efficient** than AMT

## ğŸ§ª Common Traps

- Donâ€™t tune 5+ hyperparameters at once
- Donâ€™t allow full concurrency if learning-as-you-go is expected
- Donâ€™t use linear scale for parameters ranging over **multiple orders of magnitude**
