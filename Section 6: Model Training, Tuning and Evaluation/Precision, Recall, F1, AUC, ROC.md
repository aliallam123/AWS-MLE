# Model Evaluation Metrics â€“ AWS MLE-A Exam

## âœ… Answering Strategy (Cheat-Sheet Style)

### ðŸŽ¯ Precision
- Look for: **"Minimise false positives"**
- Use case: Email spam detection, fraud alerts (wrong positive is costly)

### ðŸŽ¯ Recall
- Look for: **"Minimise false negatives"**
- Use case: Disease detection, safety alerts (missing a positive is dangerous)

### ðŸŽ¯ F1 Score
- Look for: **"Balance between precision and recall"** or **"Imbalanced classes"**
- Use when both FP and FN are important and data is skewed

### ðŸŽ¯ AUC / ROC
- Look for: **"Ranking classifier performance"**, **"Discrimination ability"**, **"Across thresholds"**
- Use when evaluating binary classifiers, especially with imbalanced datasets

## ðŸ§  Tips for the Exam

- If you see "imbalanced dataset" â†’ **prefer F1 score or AUC**
- If cost of false **positive** is high â†’ **precision**
- If cost of false **negative** is high â†’ **recall**
- ROC curve plots **True Positive Rate vs False Positive Rate**
- AUC closer to **1.0 = better**, closer to **0.5 = random guessing**
- Donâ€™t pick accuracy unless class balance is clearly stated
- Be careful with tricky wording â€” always map it back to which type of error matters more

## ðŸ“Œ When NOT to use accuracy
- Class imbalance
- Cost-sensitive applications
- Binary rare-event classification
