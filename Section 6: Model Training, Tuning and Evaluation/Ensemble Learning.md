# Ensemble Learning (Bagging vs Boosting) â€“ AWS MLE-A Exam

## ğŸ§  Key Concepts

### ğŸŸ© Bagging
- Example: **Random Forest**
- Combines multiple models trained on **bootstrapped datasets**
- **Parallel** training
- Helps **reduce overfitting**
- Use when you need **stability + regularisation**

### ğŸŸ§ Boosting
- Example: **XGBoost** (popular in SageMaker)
- Models trained **sequentially**, correcting previous errors
- Focuses on **accuracy**
- Can **overfit** if not regularised
- Strong choice when accuracy is most important

## âš–ï¸ Summary Comparison

| Feature        | Bagging               | Boosting               |
|----------------|------------------------|-------------------------|
| Training Style | Parallel               | Sequential              |
| Goal           | Reduce overfitting     | Maximise accuracy       |
| Overfitting    | Helps prevent it       | Prone if not tuned      |
| Speed          | Faster (parallelizable)| Slower (serial process) |

## ğŸ§ª Exam Tips

- **XGBoost** â†’ Think **boosting + accuracy**
- **Random Forest** â†’ Think **bagging + overfitting**
- Boosting = Better performance, more sensitive
- Bagging = More stable, easier to scale
