# Advanced SageMaker AMT â€“ AWS MLE-A Exam

## ğŸ§  Key Features

### ğŸ”¹ Early Stopping
- Stops training jobs **if no improvement** in objective metric
- Works only for models that emit metrics **each epoch**
- Enable by setting `early_stopping_type = 'Auto'`

### ğŸ”¹ Warm Start
- Reuses previous tuning jobs to speed up convergence
- Two modes:
  - **IdenticalDataAndAlgorithm** â†’ same data/algorithm
  - **TransferLearning** â†’ allows data variation

## ğŸ› ï¸ Tuning Strategies

| Strategy       | Description                                              | Notes                                    |
|----------------|----------------------------------------------------------|------------------------------------------|
| Grid Search    | Tries **all combos** (categorical only)                  | Very expensive, not scalable             |
| Random Search  | Tries **random combos**                                  | Parallelisable, fast but less optimal    |
| Bayesian       | Learns & **improves each run** (regression-based)        | Sequential (limited parallelism)         |
| Hyperband      | Early stops bad configs, auto-parallel                   | **Best choice for deep learning**, fast  |

## ğŸ“ Best Practices

- ğŸ”¢ Tune **few hyperparameters at a time**
- ğŸ“‰ Use **small value ranges** where possible
- ğŸ§® Use **log scale** for values like learning rate (0.001â€“0.1)
- ğŸš« **Donâ€™t run too many jobs concurrently** â€” reduces AMTâ€™s ability to learn

## â›” Resource Limits

- Default quotas:
  - Max parallel jobs
  - Max training jobs per tuning job
  - Max hyperparameters
- ğŸ“© Increase via **AWS support request** if needed

## ğŸ§ª Exam Tips

- If you see â€œstopped earlyâ€ â†’ answer is **early stopping**
- If using **previous jobs to restart** tuning â†’ answer is **warm start**
- For **deep learning models** + **epoch metrics** â†’ use **Hyperband**
- If parallelism is key â†’ use **random search**
- If you want **best convergence** and can run sequentially â†’ use **Bayesian**
