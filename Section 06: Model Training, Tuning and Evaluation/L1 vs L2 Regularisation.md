# L1 vs L2 Regularization â€“ AWS MLE-A Exam

## ğŸ¯ Purpose

Reduce overfitting by penalizing large weights during training.

## ğŸ” Key Differences

| Aspect         | L1 (Lasso)                          | L2 (Ridge)                         |
|----------------|-------------------------------------|------------------------------------|
| Penalty        | Sum of absolute weights             | Sum of squared weights             |
| Output         | Sparse (some weights = 0)           | Dense (no weights = 0)             |
| Use Case       | Feature selection                   | Keep all features, shrink weights  |
| Performance    | Computationally heavier             | More efficient                     |

## ğŸ§  Tips

- Use **L1** when you want **fewer features** â†’ automatic feature selection.
- Use **L2** when all features are important â†’ just reduce overfitting.
- Both can be combined (ElasticNet) â€” not exam-critical.
