# Model Deployment Best Practices â€“ AWS MLE-A Notes

## Core Deployment Flow
1. **Inference Container**
   - Holds inference code + model artifacts (from **Model Registry**) + features (from **Feature Store**).  
   - Deployed as an **endpoint** that applications call for predictions.  

2. **Application Integration**
   - Endpoint consumed by downstream apps.  
   - APIs can be fronted by **API Gateway** for stability.  

---

## Best Practices

### Monitoring & Security
- ğŸ“Š Use **CloudWatch, EventBridge, SNS** â†’ monitor endpoints & alerts.  
- ğŸ›¡ï¸ Detect drift/adversarial activity â†’ **SageMaker Model Monitor**.  
- ğŸ” Protect against malicious input/data poisoning.  

### Automation & Testing
- ğŸ”„ Automate CI/CD deployment â†’ **SageMaker Pipelines**.  
- ğŸ§ª Controlled rollout strategies:  
  - **Blue/Green** â†’ swap old/new endpoints.  
  - **Canary/Linear** â†’ gradually shift traffic.  
  - **A/B Testing** â†’ test multiple models in prod.  

### Deployment Options
- â˜ï¸ **Cloud Inference**:  
  - Real-time endpoints.  
  - Serverless endpoints.  
  - Asynchronous inference.  
  - Batch transform (scheduled).  
- ğŸ“± **Edge Inference**:  
  - **SageMaker Neo**, **AWS IoT Greengrass**.  
  - Useful for low-latency or offline scenarios.  
- âš™ï¸ Advanced: Multi-model & multi-container endpoints, Inference Pipelines.  

### Cost Optimization
- ğŸ’¸ Use efficient hardware:  
  - **Inferentia (Inf1/Inf2)** â†’ DL inference.  
  - **Trainium (Trn1)** â†’ model training efficiency.  
  - **Graviton3 CPUs** â†’ general compute efficiency.  
- ğŸ“‰ Rightsize fleets â†’ **SageMaker Inference Recommender**.  
- âš¡ Auto scaling for endpoints â†’ scale by load.  
- ğŸ§  Optimize models pre-deployment â†’ **Neo, TreeLite, HuggingFace Infinity**.  
- â™»ï¸ Deploy multiple models behind one endpoint for efficiency.  

### Sustainability & SLAs
- âš–ï¸ Match deployment strategy to business need:  
