# ğŸ“˜ SageMaker Built-in Algorithm: BlazingText (Exam Revision)

## âœ… Overview
- **Type**: NLP (Natural Language Processing)
- **Use Cases**:
  - **Text Classification** (supervised)
  - **Word2Vec** (unsupervised word embeddings)
- â— Not for full documents â€” supports **sentences only**

---

## ğŸ§  Modes of Operation

### ğŸ”¹ Supervised Mode (Text Classification)
- Learns to classify sentences based on labelled training data
- Use Case: tagging, info retrieval, search relevance

### ğŸ”¹ Word2Vec Mode (Embedding Layer)
- Learns **word similarities** via embeddings
- Useful in downstream tasks (e.g., sentiment, translation)
- Cannot classify â€” only provides word vectors

#### Word2Vec Training Modes:
| Mode            | Notes                                                             |
|-----------------|-------------------------------------------------------------------|
| `cbow`          | Continuous bag of words â€“ word order ignored                      |
| `skipgram`      | Order matters; works with n-grams                                 |
| `batch_skipgram`| Scalable across multiple CPU instances (no GPU)                   |

---

## ğŸ“¥ Input Formats

### ğŸ”¹ Text Classification
- One sentence per line
- First "word" must be: `__label__<label>`
- Sentence must be:
  - Lowercased
  - Tokenised (space-separated, incl. punctuation)

**Example**:
__label__4 linux ready for prime time .


### ğŸ”¹ Word2Vec
- One sentence per line
- No labels
- Plain text

### ğŸ”¹ Augmented Manifest Format
- Structured JSON-style: `"source"` and `"label"` fields

---

## ğŸ›ï¸ Important Hyperparameters

### Text Classification
- `learning_rate`
- `epochs`
- `word_ngrams`: how many words to look at together
- `vector_dim`: size of embedding vector

### Word2Vec
- `mode`: `cbow`, `skipgram`, or `batch_skipgram`
- `learning_rate`
- `window_size`: context window
- `vector_dim`
- `negative_samples`

---

## ğŸ’» Instance Support

| Mode             | Instance Recommendation                        |
|------------------|------------------------------------------------|
| **cbow / skipgram**   | P3 (single machine GPU)                       |
| **batch_skipgram**    | Multiple CPU instances (scales horizontally) |
| **Text Classification** | 
- `<2GB data`: `ml.c5.xlarge`  
- `>2GB data`: `ml.p2.xlarge` or `ml.p3.2xlarge`

---

## ğŸ“ Exam Tips
- **Supervised mode** â†’ text classification
- **Word2Vec mode** â†’ embedding only, NOT classification
- `batch_skipgram` â†’ scales with CPUs across instances
- Know the **input formats** â€“ especially `__label__` for classification
- Use **CBOW** when word order doesnâ€™t matter, **skipgram** when it does
- Small training set? â†’ use `c5` CPU
- Embedding mode? â†’ use `P3` for best performance (except batch mode)
