# ğŸ§  Encoder vs Decoder â€“ Quick Revision

## ğŸ“ Analogy: Classroom Translator

- **Encoder** = The **Listener**  
  Listens to a sentence (e.g. in French) and understands the meaning.  
  ğŸ‘‰ *"Je suis fatiguÃ©"* â†’ internal meaning: â€œTheyâ€™re tired.â€

- **Decoder** = The **Speaker**  
  Takes that meaning and generates a response (e.g. in English).  
  ğŸ‘‰ Says: â€œTheyâ€™re tired.â€

---

## ğŸ” Transformer Breakdown

| Feature         | Encoder                          | Decoder                          |
|----------------|----------------------------------|----------------------------------|
| **Role**        | Understands the **input**        | Generates the **output**         |
| **Used in**     | BERT                             | GPT                              |
| **Input**       | Full input sequence              | Previous tokens (and encoder output if used) |
| **Attention**   | Self-attention only              | Self-attention + cross-attention |
| **Output**      | Embeddings/hidden states         | Tokens, one-by-one               |

---

## ğŸ§ª Key Difference

- **Encoder**: Reads and encodes information into a **vector representation**.
- **Decoder**: Takes that representation and **generates new sequences** (e.g. words or tokens).

---

## ğŸ¤– Model Examples

| Model | Uses | Architecture |
|-------|------|--------------|
| **BERT** | Classification, Q&A | Encoder-only |
| **GPT** | Text generation      | Decoder-only |
| **T5**  | Translation, summarisation | Encoder + Decoder |

